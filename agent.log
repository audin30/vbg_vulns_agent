2025-10-30 23:11:04,097 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-30 23:11:15,384 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-30 23:11:15,386 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 97, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-30 23:11:29,344 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-30 23:11:29,346 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 97, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-30 23:12:34,508 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-30 23:12:44,916 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-30 23:12:44,920 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-30 23:12:53,619 [INFO] ðŸ‘‹ Exiting agent. Goodbye!
2025-10-30 23:13:09,068 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-30 23:13:13,991 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-30 23:13:13,994 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4-turbo` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-30 23:14:35,422 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-30 23:14:39,934 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-30 23:14:39,936 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-4-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-4-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-30 23:14:53,529 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-30 23:14:58,441 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-30 23:14:58,444 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-40-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-40-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-30 23:57:36,256 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-30 23:57:43,412 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-30 23:57:43,415 [ERROR] Unexpected error during query: Error code: 400 - {'error': {'message': "Unsupported parameter: 'stop' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'stop', 'code': 'unsupported_parameter'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'stop' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'stop', 'code': 'unsupported_parameter'}}
2025-10-30 23:58:12,466 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-30 23:58:21,606 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-30 23:58:21,609 [ERROR] Unexpected error during query: Error code: 400 - {'error': {'message': "Unsupported parameter: 'stop' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'stop', 'code': 'unsupported_parameter'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'stop' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'stop', 'code': 'unsupported_parameter'}}
2025-10-30 23:59:41,518 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-30 23:59:47,623 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-30 23:59:47,627 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-40-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-40-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-31 00:18:40,881 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-31 00:18:45,382 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-31 00:18:45,385 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-40-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-40-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-31 00:20:23,935 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-31 00:20:28,298 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-31 00:20:28,302 [ERROR] Unexpected error during query: Error code: 400 - {'error': {'message': "Unsupported parameter: 'stop' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'stop', 'code': 'unsupported_parameter'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'stop' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'stop', 'code': 'unsupported_parameter'}}
2025-10-31 00:20:42,238 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-31 00:20:46,835 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-10-31 00:20:46,839 [ERROR] Unexpected error during query: Error code: 400 - {'error': {'message': "Unsupported parameter: 'stop' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'stop', 'code': 'unsupported_parameter'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 137, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Unsupported parameter: 'stop' is not supported with this model.", 'type': 'invalid_request_error', 'param': 'stop', 'code': 'unsupported_parameter'}}
2025-10-31 00:34:09,374 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-31 00:34:13,929 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-10-31 00:34:13,932 [ERROR] Unexpected error during query: Error code: 404 - {'error': {'message': 'The model `gpt-40-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 138, in <module>
    response = agent_executor.invoke({"input": query})
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/chains/base.py", line 165, in invoke
    self._call(inputs, run_manager=run_manager)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1625, in _call
    next_step_output = self._take_next_step(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1325, in _take_next_step
    list(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 1352, in _iter_next_step
    output = self._action_agent.plan(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain/agents/agent.py", line 455, in plan
    for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3650, in stream
    yield from self.transform(iter([input]), config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3636, in transform
    yield from self._transform_stream_with_config(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 2372, in _transform_stream_with_config
    chunk: Output = context.run(next, iterator)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 3595, in _transform
    yield from final_pipeline
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1571, in transform
    for ichunk in input:
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 5927, in transform
    yield from self.bound.transform(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py", line 1589, in transform
    yield from self.stream(final, config, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py", line 522, in stream
    for chunk in self._stream(input_messages, stop=stop, **kwargs):
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py", line 1123, in _stream
    response = self.client.create(**payload)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py", line 1156, in create
    return self._post(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `gpt-40-mini` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}
2025-10-31 00:36:35,663 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-10-31 00:36:40,439 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-31 00:36:41,648 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-10-31 16:45:57,005 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 08:38:15,217 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 08:38:30,529 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 08:38:32,728 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 08:38:40,778 [INFO] ðŸ‘‹ Exiting agent. Goodbye!
2025-11-01 09:21:57,467 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 09:22:03,407 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:22:05,027 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:22:06,984 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:22:08,650 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:22:16,918 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:22:17,532 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:22:29,344 [INFO] ðŸ‘‹ Exiting agent. Goodbye!
2025-11-01 09:34:09,711 [ERROR] âŒ Unexpected error while loading data: 'asset_id'
Traceback (most recent call last):
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/app.py", line 52, in <module>
    df = correlate_data()
  File "/Users/secu/Desktop/Git/vbg_vuln_agent/tools/data_tools.py", line 53, in correlate_data
    merged = pd.merge(vulns, assets, left_on="asset_id", right_on="asset_id", how="left")
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/merge.py", line 170, in merge
    op = _MergeOperation(
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/merge.py", line 794, in __init__
    ) = self._get_merge_keys()
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/pandas/core/reshape/merge.py", line 1298, in _get_merge_keys
    right_keys.append(right._get_label_or_level_values(rk))
  File "/Users/secu/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py", line 1911, in _get_label_or_level_values
    raise KeyError(key)
KeyError: 'asset_id'
2025-11-01 09:35:39,067 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 09:35:47,103 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:35:47,930 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:36:22,770 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:36:24,275 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:36:40,248 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:36:41,578 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:36:42,943 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:36:44,853 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:36:56,629 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:36:57,857 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 09:37:08,392 [INFO] ðŸ‘‹ Exiting agent. Goodbye!
2025-11-01 10:01:30,004 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 10:01:39,420 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:01:40,469 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:01:49,536 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:01:50,912 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:02:12,294 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:02:15,194 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:02:17,215 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:02:38,616 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:02:39,593 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:04:28,123 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 10:04:34,523 [INFO] ðŸ‘‹ Exiting agent. Goodbye!
2025-11-01 10:10:28,477 [INFO] ðŸ¤– Using OpenAI Model: gpt-4o-mini
2025-11-01 10:10:30,277 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:10:30,289 [INFO] âœ… OpenAI connection verified! Test response: Pong! How can I assist you today?
2025-11-01 10:10:30,535 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 10:10:45,961 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:10:47,292 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:11:20,063 [INFO] ðŸ‘‹ Exiting agent. Goodbye!
2025-11-01 10:12:29,729 [INFO] ðŸ¤– Using OpenAI Model: gpt-4o-mini
2025-11-01 10:12:30,310 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:12:30,323 [INFO] âœ… OpenAI connection verified! Test response: Pong! How can I assist you today?
2025-11-01 10:12:30,438 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 10:12:39,610 [INFO] ðŸ‘‹ Exiting agent. Goodbye!
2025-11-01 10:14:01,309 [INFO] ðŸ¤– Using OpenAI Model: gpt-4o-mini
2025-11-01 10:14:01,859 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-01 10:14:01,867 [INFO] âœ… OpenAI connection verified! Test response: Pong! How can I assist you today?
2025-11-01 10:14:01,979 [INFO] âœ… Vulnerability Agent ready! Type 'exit' or 'quit' to stop.
2025-11-01 10:14:15,120 [INFO] ðŸ‘‹ Exiting agent. Goodbye!
2025-11-02 09:50:10,399 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:50:20,017 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 09:50:22,581 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 18:49:47,385 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 18:50:05,121 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
2025-11-02 18:50:06,167 [INFO] Exported 414 rows to output/vulns_High_414rows_2025-11-02_185006.csv
2025-11-02 18:50:06,196 [INFO] Listed vulnerabilities (High) | Rows: 414
2025-11-02 18:50:08,038 [INFO] HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
